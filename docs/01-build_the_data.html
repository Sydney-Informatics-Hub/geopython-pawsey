<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />




<title>01-build_the_data.utf8.md</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/simplex.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>

<link rel="stylesheet" href="lesson.css" type="text/css" />



<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 41px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 46px;
  margin-top: -46px;
}
.section h2 {
  padding-top: 46px;
  margin-top: -46px;
}
.section h3 {
  padding-top: 46px;
  margin-top: -46px;
}
.section h4 {
  padding-top: 46px;
  margin-top: -46px;
}
.section h5 {
  padding-top: 46px;
  margin-top: -46px;
}
.section h6 {
  padding-top: 46px;
  margin-top: -46px;
}
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #ffffff;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  background: white;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>

<!-- code folding -->



<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_').toLowerCase();
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}


.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
  padding-left: 25px;
  text-indent: 0;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Home</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Setup
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="setup.html">Setup</a>
    </li>
    <li>
      <a href="AWSinstuctionsStudents.html">AWS Backup instructions</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Sessions
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="00-intro.html">Introduction</a>
    </li>
    <li>
      <a href="01-build_the_data.html">Building an ML dataset</a>
    </li>
    <li>
      <a href="02-explore_the_data.html">Exploring the data</a>
    </li>
    <li>
      <a href="03a-MachineLearing.html">Machine Learning</a>
    </li>
  </ul>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">




</div>


<div id="building-machine-learning-datasets" class="section level1">
<h1>Building Machine Learning Datasets</h1>
<p>All machine learning problems begin with a dataset, and before we can perform any kind of inference on that dataset we must create/wrangle/build it. This is often the most time-consuming and hard part of a successful machine learning workflow. There is no single procedure here, as all data is different, although there are a few simple methods we can take to make a useful dataset.</p>
<p>We will be using data from a submitted Manuscript (Butterworth and Barnett-Moore 2020) which was a finalist in the <a href="https://unearthed.solutions/u/competitions/exploresa">Unearthed, ExploreSA: Gawler Challenge</a>. You can visit the <a href="https://github.com/natbutter/gawler-exploration">original repo here</a></p>
<pre class="python"><code>import shapefile
import pandas as pd
import numpy as np
import scipy
from scipy import io
import time
import matplotlib.pyplot as plt
import cartopy.crs as ccrs</code></pre>
<pre class="python"><code>#Import libraries for data manipulations
import pandas as pd
import numpy as np
import random
import scipy
from scipy import io

#Import libraries for plotting
import matplotlib.pyplot as plt
import matplotlib.ticker as mticker
import cartopy.crs as ccrs
from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER
from mpl_toolkits import mplot3d
import matplotlib.mlab as ml
from cartopy.io.img_tiles import Stamen
from numpy import linspace, meshgrid
from matplotlib.mlab import griddata
from matplotlib.path import Path
from matplotlib.patches import PathPatch

#Import libraries for tif, shapefile, and geodata manipulations
import shapefile
from shapely.geometry import Point
from shapely.geometry import shape

#Import Machine Learning libraries
#from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn import preprocessing
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler, OneHotEncoder


#Import libraries for multi-threading capabilities
#from dask import delayed,compute
#from dask.distributed import Client, progress
import time</code></pre>
<pre class="python"><code>#Define simple helper functions used in workflow

def coregPoint(point,data,region):
    &#39;&#39;&#39;
    Finds the nearest neighbour to a point from a bunch of other points
    point - array([longitude,latitude])
    data - array
    region - integer, same units as data
    &#39;&#39;&#39;
    tree = scipy.spatial.cKDTree(data)
    dists, indexes = tree.query(point,k=1,distance_upper_bound=region) 

    if indexes==len(data):
        return &#39;inf&#39;
    else:
        return (indexes,dists)
    
#
def points_in_circle(circle, arr):
    &#39;&#39;&#39;
    A generator to return all points whose indices are within given circle.
    http://stackoverflow.com/a/2774284
    Warning: If a point is near the the edges of the raster it will not loop 
    around to the other side of the raster!
    &#39;&#39;&#39;
    i0,j0,r = circle

    for i in range(intceil(i0-r),intceil(i0+r)):
        ri = np.sqrt(r**2-(i-i0)**2)
        for j in range(intceil(j0-ri),intceil(j0+ri)):
            if (i &gt;= 0 and i &lt; len(arr[:,0])) and (j&gt;=0 and j &lt; len(arr[0,:])):
                yield arr[i][j]

#            
def intceil(x):
    return int(np.ceil(x))                                            

#
def coregRaster(point,data,region):
    &#39;&#39;&#39;
    Finds the mean value of a raster, around a point with a specified radius.
    point - array([longitude,latitude])
    data - array
    region - integer, same units as data
    &#39;&#39;&#39;
    i0=point[1]
    j0=point[0]
    r=region #In units of degrees
    pts_iterator = points_in_circle((i0,j0,region), data)
    pts = np.array(list(pts_iterator))
    #remove values outside the region which for there is no data (0.0).
    #print(pts)
    pts = pts[pts != 0.]
    if np.isnan(np.nanmean(pts)):
        #print(point,&quot;nan&quot;)
        #pts=np.median(data)
        pts=-9999.
        #print(&quot;returning&quot;,pts)

    return(np.nanmean(pts))

#Make a function that can turn point arrays into a full meshgrid
def grid(x, y, z, resX=100, resY=100):
    &quot;Convert 3 column data to matplotlib grid&quot;
    xi = linspace(min(x), max(x), resX)
    yi = linspace(min(y), max(y), resY)
    Z = griddata(x, y, z, xi, yi,interp=&#39;linear&#39;)
    X, Y = meshgrid(xi, yi)
    return X, Y, Z

#Define a function to read the netcdf files
def readnc(filename):
    tic=time.time()
    rasterfile=filename
    data = scipy.io.netcdf_file(rasterfile,&#39;r&#39;)
    xdata=data.variables[&#39;lon&#39;][:]
    ydata=data.variables[&#39;lat&#39;][:]
    zdata=np.array(data.variables[&#39;Band1&#39;][:])

    toc=time.time()
    print(rasterfile, &quot;in&quot;, toc-tic)
    print(&quot;spacing x&quot;, xdata[2]-xdata[1], &quot;y&quot;, ydata[2]-ydata[1], np.shape(zdata),np.min(xdata),np.max(xdata),np.min(ydata),np.max(ydata))

    return(xdata,ydata,zdata)

#Define a function to find what polygon a point lives inside (speed imporivements can be made here)
def shapeExplore(point,shapes,recs,record):
    #&#39;record&#39; is the column index you want returned
    for i in range(len(shapes)):
        boundary = shapes[i]
        if Point((point.lon,point.lat)).within(shape(boundary)):
            return(recs[i][record])
    #if you have been through the loop with no result
    return(-9999.)</code></pre>
</div>
<div id="goal-create-a-table-of-data-containing-targets-and-predictor-variables" class="section level1">
<h1>Goal: Create a table of data containing “targets” and “predictor variables”</h1>
<p>The targets in an ML context can be a simple binary 1 or 0, or could be some category. It is the “feature” of a dataset that we want to learn about!</p>
<p>The “predictor/feature variables” are the quatities that may have some causal relationship with the the targets.</p>
<div id="deposit-locations---mine-and-mineral-occurances" class="section level3">
<h3>Deposit locations - mine and mineral occurances</h3>
<p>The most important dataset for this workflow is the currently known locations of mineral occurences. Using the data we already know about these known-deposits we will build a model to predict where future occurences will be.</p>
<pre class="python"><code>#Set the filename
mineshape=&quot;data/MinesMinerals/mines_and_mineral_occurrences_all.shp&quot;

#Set shapefile attributes and assign
sf = shapefile.Reader(mineshape)
fields = [x[0] for x in sf.fields][1:]
records = sf.records()
shps = [s.points for s in sf.shapes()]

#write into a dataframe fo easy use
df = pd.DataFrame(columns=fields, data=records)</code></pre>
<pre class="python"><code>#See what the dataframe looks like
df</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th>
</th>
<th>
MINDEP_NO
</th>
<th>
DEP_NAME
</th>
<th>
REFERENCE
</th>
<th>
COMM_CODE
</th>
<th>
COMMODS
</th>
<th>
COMMOD_MAJ
</th>
<th>
COMM_SPECS
</th>
<th>
GCHEM_ASSC
</th>
<th>
DISC_YEAR
</th>
<th>
CLASS_CODE
</th>
<th>
…
</th>
<th>
NORTHING
</th>
<th>
ZONE
</th>
<th>
LONGITUDE
</th>
<th>
LATITUDE
</th>
<th>
SVY_METHOD
</th>
<th>
HORZ_ACC
</th>
<th>
SRCE_MAP
</th>
<th>
SRCE_CNTRE
</th>
<th>
COMMENTS
</th>
<th>
O_MAP_SYMB
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
0
</th>
<td>
5219
</td>
<td>
MOUNT DAVIES NO.2A
</td>
<td>
RB 65/80
</td>
<td>
Ni
</td>
<td>
Nickel
</td>
<td>
Ni
</td>
<td>
ELMT
</td>
<td>
</td>
<td>
1893.0
</td>
<td>
OCCURRENCE
</td>
<td>
…
</td>
<td>
7112524.68
</td>
<td>
52
</td>
<td>
129.200549
</td>
<td>
-26.106335
</td>
<td>
Digitised
</td>
<td>
2000.0
</td>
<td>
500k meis
</td>
<td>
</td>
<td>
</td>
<td>
T
</td>
</tr>
<tr>
<th>
1
</th>
<td>
52
</td>
<td>
ONE STONE
</td>
<td>
MRR 138
</td>
<td>
Ni
</td>
<td>
Nickel
</td>
<td>
Ni
</td>
<td>
ELMT
</td>
<td>
Ni-Cr
</td>
<td>
1975.0
</td>
<td>
OCCURRENCE
</td>
<td>
…
</td>
<td>
7110551.56
</td>
<td>
53
</td>
<td>
132.775358
</td>
<td>
-26.107124
</td>
<td>
Sourced from documents (PLANS, ENV, RB,etc)
</td>
<td>
500.0
</td>
<td>
71-385
</td>
<td>
</td>
<td>
</td>
<td>
T/td&gt;
</tr>
<pre><code>&lt;tr&gt;
  &lt;th&gt;2&lt;/th&gt;
  &lt;td&gt;8314&lt;/td&gt;
  &lt;td&gt;HINCKLEY RANGE&lt;/td&gt;
  &lt;td&gt;&lt;/td&gt;
  &lt;td&gt;Fe&lt;/td&gt;
  &lt;td&gt;Iron&lt;/td&gt;
  &lt;td&gt;Fe&lt;/td&gt;
  &lt;td&gt;ELMT&lt;/td&gt;
  &lt;td&gt;&lt;/td&gt;
  &lt;td&gt;1961.0&lt;/td&gt;
  &lt;td&gt;OCCURRENCE&lt;/td&gt;
  &lt;td&gt;...&lt;/td&gt;
  &lt;td&gt;7111381.52&lt;/td&gt;
  &lt;td&gt;52&lt;/td&gt;
  &lt;td&gt;129.101731&lt;/td&gt;
  &lt;td&gt;-26.116761&lt;/td&gt;
  &lt;td&gt;Sourced from documents (PLANS, ENV, RB,etc)&lt;/td&gt;
  &lt;td&gt;500.0&lt;/td&gt;
  &lt;td&gt;&lt;/td&gt;
  &lt;td&gt;&lt;/td&gt;
  &lt;td&gt;&lt;/td&gt;
  &lt;td&gt;Mg&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;3&lt;/th&gt;
  &lt;td&gt;69&lt;/td&gt;
  &lt;td&gt;KALKA&lt;/td&gt;
  &lt;td&gt;RB 91/103&lt;/td&gt;
  &lt;td&gt;V, ILM&lt;/td&gt;
  &lt;td&gt;Vanadium, Ilmenite&lt;/td&gt;
  &lt;td&gt;V&lt;/td&gt;
  &lt;td&gt;ELMT&lt;/td&gt;
  &lt;td&gt;Fe-V-Ti&lt;/td&gt;
  &lt;td&gt;1968.0&lt;/td&gt;
  &lt;td&gt;OCCURRENCE&lt;/td&gt;
  &lt;td&gt;...&lt;/td&gt;
  &lt;td&gt;7110521.49&lt;/td&gt;
  &lt;td&gt;52&lt;/td&gt;
  &lt;td&gt;129.116042&lt;/td&gt;
  &lt;td&gt;-26.124516&lt;/td&gt;
  &lt;td&gt;(DISUSED) Map Plot&lt;/td&gt;
  &lt;td&gt;100.0&lt;/td&gt;
  &lt;td&gt;1 MILE&lt;/td&gt;
  &lt;td&gt;mgt polygon on digital map&lt;/td&gt;
  &lt;td&gt;&lt;/td&gt;
  &lt;td&gt;Mg2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;4&lt;/th&gt;
  &lt;td&gt;65&lt;/td&gt;
  &lt;td&gt;ECHIDNA&lt;/td&gt;
  &lt;td&gt;RB 91/103&lt;/td&gt;
  &lt;td&gt;Ni&lt;/td&gt;
  &lt;td&gt;Nickel&lt;/td&gt;
  &lt;td&gt;Ni&lt;/td&gt;
  &lt;td&gt;ELMT&lt;/td&gt;
  &lt;td&gt;Ni&lt;/td&gt;
  &lt;td&gt;1991.0&lt;/td&gt;
  &lt;td&gt;OCCURRENCE&lt;/td&gt;
  &lt;td&gt;...&lt;/td&gt;
  &lt;td&gt;7108531.53&lt;/td&gt;
  &lt;td&gt;53&lt;/td&gt;
  &lt;td&gt;132.770515&lt;/td&gt;
  &lt;td&gt;-26.125281&lt;/td&gt;
  &lt;td&gt;(DISUSED) Map Plot&lt;/td&gt;
  &lt;td&gt;20.0&lt;/td&gt;
  &lt;td&gt;50K GEOL&lt;/td&gt;
  &lt;td&gt;DH ECHIDNA PROSPECT&lt;/td&gt;
  &lt;td&gt;&lt;/td&gt;
  &lt;td&gt;LMb&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;...&lt;/th&gt;
  &lt;td&gt;...&lt;/td&gt;
  &lt;td&gt;...&lt;/td&gt;
  &lt;td&gt;...&lt;/td&gt;
  &lt;td&gt;...&lt;/td&gt;
  &lt;td&gt;...&lt;/td&gt;
  &lt;td&gt;...&lt;/td&gt;
  &lt;td&gt;...&lt;/td&gt;
  &lt;td&gt;...&lt;/td&gt;
  &lt;td&gt;...&lt;/td&gt;
  &lt;td&gt;...&lt;/td&gt;
  &lt;td&gt;...&lt;/td&gt;
  &lt;td&gt;...&lt;/td&gt;
  &lt;td&gt;...&lt;/td&gt;
  &lt;td&gt;...&lt;/td&gt;
  &lt;td&gt;...&lt;/td&gt;
  &lt;td&gt;...&lt;/td&gt;
  &lt;td&gt;...&lt;/td&gt;
  &lt;td&gt;...&lt;/td&gt;
  &lt;td&gt;...&lt;/td&gt;
  &lt;td&gt;...&lt;/td&gt;
  &lt;td&gt;...&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;8672&lt;/th&gt;
  &lt;td&gt;6937&lt;/td&gt;
  &lt;td&gt;YARINGA&lt;/td&gt;
  &lt;td&gt;RB 43/94&lt;/td&gt;
  &lt;td&gt;QTZE&lt;/td&gt;
  &lt;td&gt;Quartzite&lt;/td&gt;
  &lt;td&gt;QTZE&lt;/td&gt;
  &lt;td&gt;ROCK&lt;/td&gt;
  &lt;td&gt;&lt;/td&gt;
  &lt;td&gt;1956.0&lt;/td&gt;
  &lt;td&gt;OCCURRENCE&lt;/td&gt;
  &lt;td&gt;...&lt;/td&gt;
  &lt;td&gt;6066051.52&lt;/td&gt;
  &lt;td&gt;54&lt;/td&gt;
  &lt;td&gt;138.254441&lt;/td&gt;
  &lt;td&gt;-35.517924&lt;/td&gt;
  &lt;td&gt;Google Earth image&lt;/td&gt;
  &lt;td&gt;200.0&lt;/td&gt;
  &lt;td&gt;50k moc&lt;/td&gt;
  &lt;td&gt;fenced yard&lt;/td&gt;
  &lt;td&gt;&lt;/td&gt;
  &lt;td&gt;Eec&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;8673&lt;/th&gt;
  &lt;td&gt;4729&lt;/td&gt;
  &lt;td&gt;WELCHS&lt;/td&gt;
  &lt;td&gt;MSC #19&lt;/td&gt;
  &lt;td&gt;SCHT&lt;/td&gt;
  &lt;td&gt;Schist&lt;/td&gt;
  &lt;td&gt;SCHT&lt;/td&gt;
  &lt;td&gt;ROCK&lt;/td&gt;
  &lt;td&gt;&lt;/td&gt;
  &lt;td&gt;1930.0&lt;/td&gt;
  &lt;td&gt;OCCURRENCE&lt;/td&gt;
  &lt;td&gt;...&lt;/td&gt;
  &lt;td&gt;6066681.48&lt;/td&gt;
  &lt;td&gt;54&lt;/td&gt;
  &lt;td&gt;138.648619&lt;/td&gt;
  &lt;td&gt;-35.520578&lt;/td&gt;
  &lt;td&gt;Digital Image&lt;/td&gt;
  &lt;td&gt;20.0&lt;/td&gt;
  &lt;td&gt;50k topo&lt;/td&gt;
  &lt;td&gt;&lt;/td&gt;
  &lt;td&gt;&lt;/td&gt;
  &lt;td&gt;Elb&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;8674&lt;/th&gt;
  &lt;td&gt;4718&lt;/td&gt;
  &lt;td&gt;ARCADIAN&lt;/td&gt;
  &lt;td&gt;MSC #2&lt;/td&gt;
  &lt;td&gt;CLAY&lt;/td&gt;
  &lt;td&gt;Clay&lt;/td&gt;
  &lt;td&gt;CLAY&lt;/td&gt;
  &lt;td&gt;ROCK&lt;/td&gt;
  &lt;td&gt;&lt;/td&gt;
  &lt;td&gt;1921.0&lt;/td&gt;
  &lt;td&gt;DEPOSIT&lt;/td&gt;
  &lt;td&gt;...&lt;/td&gt;
  &lt;td&gt;6066561.56&lt;/td&gt;
  &lt;td&gt;54&lt;/td&gt;
  &lt;td&gt;138.660599&lt;/td&gt;
  &lt;td&gt;-35.521892&lt;/td&gt;
  &lt;td&gt;Digital Image&lt;/td&gt;
  &lt;td&gt;5.0&lt;/td&gt;
  &lt;td&gt;Plan 1951-0327&lt;/td&gt;
  &lt;td&gt;Pit&lt;/td&gt;
  &lt;td&gt;&lt;/td&gt;
  &lt;td&gt;Q&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;8675&lt;/th&gt;
  &lt;td&gt;1436&lt;/td&gt;
  &lt;td&gt;MCDONALD&lt;/td&gt;
  &lt;td&gt;MSC #7&lt;/td&gt;
  &lt;td&gt;Au&lt;/td&gt;
  &lt;td&gt;Gold&lt;/td&gt;
  &lt;td&gt;Au&lt;/td&gt;
  &lt;td&gt;ELMT&lt;/td&gt;
  &lt;td&gt;Au&lt;/td&gt;
  &lt;td&gt;1901.0&lt;/td&gt;
  &lt;td&gt;OCCURRENCE&lt;/td&gt;
  &lt;td&gt;...&lt;/td&gt;
  &lt;td&gt;6065991.54&lt;/td&gt;
  &lt;td&gt;54&lt;/td&gt;
  &lt;td&gt;138.436645&lt;/td&gt;
  &lt;td&gt;-35.522477&lt;/td&gt;
  &lt;td&gt;Google Earth image&lt;/td&gt;
  &lt;td&gt;200.0&lt;/td&gt;
  &lt;td&gt;50k moc&lt;/td&gt;
  &lt;td&gt;qz float&lt;/td&gt;
  &lt;td&gt;&lt;/td&gt;
  &lt;td&gt;qz&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;8676&lt;/th&gt;
  &lt;td&gt;8934&lt;/td&gt;
  &lt;td&gt;FAIRFIELD FARM&lt;/td&gt;
  &lt;td&gt;&lt;/td&gt;
  &lt;td&gt;SAND&lt;/td&gt;
  &lt;td&gt;Sand&lt;/td&gt;
  &lt;td&gt;SAND&lt;/td&gt;
  &lt;td&gt;ROCK&lt;/td&gt;
  &lt;td&gt;&lt;/td&gt;
  &lt;td&gt;1980.0&lt;/td&gt;
  &lt;td&gt;OCCURRENCE&lt;/td&gt;
  &lt;td&gt;...&lt;/td&gt;
  &lt;td&gt;6066871.52&lt;/td&gt;
  &lt;td&gt;54&lt;/td&gt;
  &lt;td&gt;138.862467&lt;/td&gt;
  &lt;td&gt;-35.522846&lt;/td&gt;
  &lt;td&gt;Google Earth image&lt;/td&gt;
  &lt;td&gt;20.0&lt;/td&gt;
  &lt;td&gt;&lt;/td&gt;
  &lt;td&gt;pit&lt;/td&gt;
  &lt;td&gt;&lt;/td&gt;
  &lt;td&gt;Q&lt;/td&gt;
&lt;/tr&gt;</code></pre>
</tbody>
</table>
<p>
8677 rows × 43 columns
</p>
</div>
<pre class="python"><code>#We are building a model to target the Gawler region specifically.
#Load in the Gawler target region boundary
gawlshape=&quot;data/SA/SA_STATE_POLYGON_shp&quot;
shapeRead = shapefile.Reader(gawlshape)
shapes  = shapeRead.shapes()

#Save the boundary xy pairs in arrays we will use throughout the workflow
xval = [x[0] for x in shapes[1].points]
yval = [x[1] for x in shapes[1].points]</code></pre>
</div>
<div id="set-the-commodity-we-want-to-target" class="section level3">
<h3>Set the commodity we want to target</h3>
<pre class="python"><code>commname=&#39;Mn&#39;</code></pre>
<pre class="python"><code>#Pull our all the occurences of the commodity and go from there
comm=df[df[&#39;COMM_CODE&#39;].str.contains(commname)]
comm=comm.reset_index(drop=True)
print(&quot;Shape of &quot;+ commname, comm.shape)

#Can make simple subsets of the data here as needed
#commsig=comm[comm.SIZE_VAL!=&quot;Low Significance&quot;]
#comm=comm[comm.SIZE_VAL!=&quot;Low Significance&quot;]
#comm=comm[comm.COX_CLASS == &quot;Olympic Dam Cu-U-Au&quot;]
#comm=comm[(comm.lon&lt;max(xval)) &amp; (comm.lon&gt;min(xval)) &amp; (comm.lat&gt;min(yval)) &amp; (comm.lat&lt;max(yval))]

#Can save subset to a file
#comm.to_csv(&quot;copper-deposits.csv&quot;)</code></pre>
<pre><code>Shape of Mn (115, 43)</code></pre>
<pre class="python"><code>comm</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th>
</th>
<th>
MINDEP_NO
</th>
<th>
DEP_NAME
</th>
<th>
REFERENCE
</th>
<th>
COMM_CODE
</th>
<th>
COMMODS
</th>
<th>
COMMOD_MAJ
</th>
<th>
COMM_SPECS
</th>
<th>
GCHEM_ASSC
</th>
<th>
DISC_YEAR
</th>
<th>
CLASS_CODE
</th>
<th>
…
</th>
<th>
NORTHING
</th>
<th>
ZONE
</th>
<th>
LONGITUDE
</th>
<th>
LATITUDE
</th>
<th>
SVY_METHOD
</th>
<th>
HORZ_ACC
</th>
<th>
SRCE_MAP
</th>
<th>
SRCE_CNTRE
</th>
<th>
COMMENTS
</th>
<th>
O_MAP_SYMB
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
0
</th>
<td>
8488
</td>
<td>
WESTERN SPUR
</td>
<td>
Env 3959
</td>
<td>
Fe, Mn
</td>
<td>
Iron, Manganese
</td>
<td>
Fe
</td>
<td>
ELMT
</td>
<td>
</td>
<td>
1981.0
</td>
<td>
PROSPECT
</td>
<td>
…
</td>
<td>
6693381.50
</td>
<td>
54
</td>
<td>
139.179436
</td>
<td>
-29.877637
</td>
<td>
Sourced from documents (PLANS, ENV, RB,etc)
</td>
<td>
50.0
</td>
<td>
</td>
<td>
near hole VP 2
</td>
<td>
</td>
<td>
Nww
</td>
</tr>
<tr>
<th>
1
</th>
<td>
4184
</td>
<td>
WILKOWIE HUT
</td>
<td>
Env 3959
</td>
<td>
Mn
</td>
<td>
Manganese
</td>
<td>
Mn
</td>
<td>
ELMT
</td>
<td>
</td>
<td>
1980.0
</td>
<td>
OCCURRENCE
</td>
<td>
…
</td>
<td>
6669627.24
</td>
<td>
54
</td>
<td>
138.808767
</td>
<td>
-30.086296
</td>
<td>
Google Earth image
</td>
<td>
20.0
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
Q
</td>
</tr>
<tr>
<th>
2
</th>
<td>
4221
</td>
<td>
DEPOT SPRINGS EAST
</td>
<td>
MSC #85
</td>
<td>
Mn
</td>
<td>
Manganese
</td>
<td>
Mn
</td>
<td>
ELMT
</td>
<td>
</td>
<td>
1949.0
</td>
<td>
OCCURRENCE
</td>
<td>
…
</td>
<td>
6629681.53
</td>
<td>
54
</td>
<td>
138.752281
</td>
<td>
-30.445684
</td>
<td>
Google Earth image
</td>
<td>
200.0
</td>
<td>
250k meis
</td>
<td>
pit?
</td>
<td>
1.7km E of Depot Springs
</td>
<td>
dw
</td>
</tr>
<tr>
<th>
3
</th>
<td>
8350
</td>
<td>
JUBILEE
</td>
<td>
RB 26/111
</td>
<td>
Mn
</td>
<td>
Manganese
</td>
<td>
Mn
</td>
<td>
ELMT
</td>
<td>
</td>
<td>
1949.0
</td>
<td>
OCCURRENCE
</td>
<td>
…
</td>
<td>
6619531.54
</td>
<td>
54
</td>
<td>
138.530506
</td>
<td>
-30.533225
</td>
<td>
Sourced from documents (PLANS, ENV, RB,etc)
</td>
<td>
200.0
</td>
<td>
plan 50-471
</td>
<td>
</td>
<td>
plan 50-471, MR 91, pp 205
</td>
<td>
dw
</td>
</tr>
<tr>
<th>
4
</th>
<td>
8349
</td>
<td>
STUART CREEK
</td>
<td>
RB 26/111
</td>
<td>
Mn
</td>
<td>
Manganese
</td>
<td>
Mn
</td>
<td>
ELMT
</td>
<td>
</td>
<td>
1949.0
</td>
<td>
OCCURRENCE
</td>
<td>
…
</td>
<td>
6616651.50
</td>
<td>
54
</td>
<td>
138.887019
</td>
<td>
-30.565479
</td>
<td>
Google Earth image
</td>
<td>
200.0
</td>
<td>
</td>
<td>
dark o/c
</td>
<td>
Mining review 091, pp 205.
</td>
<td>
dw
</td>
</tr>
<tr>
<th>
…
</th>
<td>
…
</td>
<td>
…
</td>
<td>
…
</td>
<td>
…
</td>
<td>
…
</td>
<td>
…
</td>
<td>
…
</td>
<td>
…
</td>
<td>
…
</td>
<td>
…
</td>
<td>
…
</td>
<td>
…
</td>
<td>
…
</td>
<td>
…
</td>
<td>
…
</td>
<td>
…
</td>
<td>
…
</td>
<td>
…
</td>
<td>
…
</td>
<td>
…
</td>
<td>
…
</td>
</tr>
<tr>
<th>
110
</th>
<td>
254
</td>
<td>
TUMBY SOUTH
</td>
<td>
Bull 37
</td>
<td>
Cu, Mn
</td>
<td>
Copper, Manganese
</td>
<td>
Cu
</td>
<td>
ELMT
</td>
<td>
</td>
<td>
1899.0
</td>
<td>
OCCURRENCE
</td>
<td>
…
</td>
<td>
6200974.46
</td>
<td>
53
</td>
<td>
136.059715
</td>
<td>
-34.327929
</td>
<td>
GPS Standalone Navigational
</td>
<td>
10.0
</td>
<td>
59-91
</td>
<td>
Stephen Shaft
</td>
<td>
</td>
<td>
TQr
</td>
</tr>
<tr>
<th>
111
</th>
<td>
533
</td>
<td>
CUTTLEFISH
</td>
<td>
MSC #19
</td>
<td>
Mn
</td>
<td>
Manganese
</td>
<td>
Mn
</td>
<td>
ELMT
</td>
<td>
Mn-(P)
</td>
<td>
1897.0
</td>
<td>
OCCURRENCE
</td>
<td>
…
</td>
<td>
6041551.48
</td>
<td>
54
</td>
<td>
138.016821
</td>
<td>
-35.733084
</td>
<td>
GPS Standalone Navigational
</td>
<td>
10.0
</td>
<td>
</td>
<td>
adit
</td>
<td>
sample # 54561, Fe-rich o/c,
</td>
<td>
Nni
</td>
</tr>
<tr>
<th>
112
</th>
<td>
1644
</td>
<td>
NEALES FLAT
</td>
<td>
RB 56/127
</td>
<td>
Mn
</td>
<td>
Manganese
</td>
<td>
Mn
</td>
<td>
ELMT
</td>
<td>
Mn
</td>
<td>
1962.0
</td>
<td>
OCCURRENCE
</td>
<td>
…
</td>
<td>
6208721.50
</td>
<td>
54
</td>
<td>
139.250036
</td>
<td>
-34.250155
</td>
<td>
Google Earth image
</td>
<td>
50.0
</td>
<td>
Plans FO1035 and S03399
</td>
<td>
rough centre of MC
</td>
<td>
</td>
<td>
go
</td>
</tr>
<tr>
<th>
113
</th>
<td>
5031
</td>
<td>
ROCK VALLEY
</td>
<td>
RB 61/121
</td>
<td>
Fe, Mn
</td>
<td>
Iron, Manganese
</td>
<td>
Fe
</td>
<td>
ELMT
</td>
<td>
Fe-Mn
</td>
<td>
1961.0
</td>
<td>
OCCURRENCE
</td>
<td>
…
</td>
<td>
6190251.47
</td>
<td>
53
</td>
<td>
135.905480
</td>
<td>
-34.425866
</td>
<td>
Sourced from documents (PLANS, ENV, RB,etc)
</td>
<td>
50.0
</td>
<td>
</td>
<td>
near hole Rock Valley RD 1
</td>
<td>
</td>
<td>
dw
</td>
</tr>
<tr>
<th>
114
</th>
<td>
255
</td>
<td>
WHITE FLAT
</td>
<td>
RB 81/66
</td>
<td>
Mn
</td>
<td>
Manganese
</td>
<td>
Mn
</td>
<td>
ELMT
</td>
<td>
Mn
</td>
<td>
1968.0
</td>
<td>
PROSPECT
</td>
<td>
…
</td>
<td>
6181001.47
</td>
<td>
53
</td>
<td>
135.835578
</td>
<td>
-34.509779
</td>
<td>
Sourced from documents (PLANS, ENV, RB,etc)
</td>
<td>
100.0
</td>
<td>
Plan !981-00119
</td>
<td>
PIT
</td>
<td>
</td>
<td>
TQr
</td>
</tr>
</tbody>
</table>
<p>
115 rows × 43 columns
</p>
</div>
</div>
<div id="wrangle-the-geophysical-and-geological-datasets" class="section level2">
<h2>Wrangle the geophysical and geological datasets</h2>
<p>Each geophysical dataset could offer instight into various commodities. Here we load in the pre-processed datasets and prepare them for further manipulations, data-mining, and machine learning.</p>
<div id="resistivity-xyz-data" class="section level3">
<h3>Resistivity xyz data</h3>
<pre class="python"><code>#Read in the data
data_res=pd.read_csv(&quot;data/AusLAMP_MT_Gawler_25.xyzr&quot;,
                     sep=&#39;,&#39;,header=0,names=[&#39;lat&#39;,&#39;lon&#39;,&#39;depth&#39;,&#39;resistivity&#39;])
data_res</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th>
</th>
<th>
lat
</th>
<th>
lon
</th>
<th>
depth
</th>
<th>
resistivity
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
0
</th>
<td>
-27.363931
</td>
<td>
128.680796
</td>
<td>
-25.0
</td>
<td>
2.0007
</td>
</tr>
<tr>
<th>
1
</th>
<td>
-27.659362
</td>
<td>
128.662322
</td>
<td>
-25.0
</td>
<td>
1.9979
</td>
</tr>
<tr>
<th>
2
</th>
<td>
-27.886602
</td>
<td>
128.647965
</td>
<td>
-25.0
</td>
<td>
1.9948
</td>
</tr>
<tr>
<th>
3
</th>
<td>
-28.061394
</td>
<td>
128.636833
</td>
<td>
-25.0
</td>
<td>
1.9918
</td>
</tr>
<tr>
<th>
4
</th>
<td>
-28.195844
</td>
<td>
128.628217
</td>
<td>
-25.0
</td>
<td>
1.9885
</td>
</tr>
<tr>
<th>
…
</th>
<td>
…
</td>
<td>
…
</td>
<td>
…
</td>
<td>
…
</td>
</tr>
<tr>
<th>
11003
</th>
<td>
-35.127716
</td>
<td>
142.399588
</td>
<td>
-25.0
</td>
<td>
2.0079
</td>
</tr>
<tr>
<th>
11004
</th>
<td>
-35.230939
</td>
<td>
142.408396
</td>
<td>
-25.0
</td>
<td>
2.0084
</td>
</tr>
<tr>
<th>
11005
</th>
<td>
-35.365124
</td>
<td>
142.419903
</td>
<td>
-25.0
</td>
<td>
2.0085
</td>
</tr>
<tr>
<th>
11006
</th>
<td>
-35.539556
</td>
<td>
142.434958
</td>
<td>
-25.0
</td>
<td>
2.0076
</td>
</tr>
<tr>
<th>
11007
</th>
<td>
-35.766303
</td>
<td>
142.454694
</td>
<td>
-25.0
</td>
<td>
2.0049
</td>
</tr>
</tbody>
</table>
<p>
11008 rows × 4 columns
</p>
</div>
<pre class="python"><code>im=plt.scatter(data_res.lon,data_res.lat,c=data_res.resistivity)
plt.plot(xval,yval,&#39;grey&#39;,linewidth=0.5,label=&#39;SA&#39;)
plt.plot(comm.LONGITUDE, comm.LATITUDE, marker=&#39;o&#39;, linestyle=&#39;&#39;, color=&#39;y&#39;)
plt.colorbar(im)</code></pre>
<pre><code>&lt;matplotlib.colorbar.Colorbar at 0x22f4f09f508&gt;</code></pre>
<div class="figure">
<img src="01-build_the_data_files/01-build_the_data_15_1.png" alt="" />
<p class="caption">png</p>
</div>
</div>
<div id="faults-and-dykes-vector-polylines" class="section level3">
<h3>Faults and dykes vector polylines</h3>
<pre class="python"><code>#Get fault data neo
faultshape=&quot;data/Faults/Faults.shp&quot;
shapeRead = shapefile.Reader(faultshape)
shapes  = shapeRead.shapes()
Nshp    = len(shapes)

faultsNeo=[]
for i in range(0,Nshp):
    for j in shapes[i].points:
        faultsNeo.append([j[0],j[1]])
faultsNeo=np.array(faultsNeo)
p
faultsNeo</code></pre>
<pre><code>array([[133.46269605, -27.41825034],
       [133.46770683, -27.42062991],
       [133.4723624 , -27.42259841],
       ...,
       [138.44613353, -35.36560605],
       [138.44160669, -35.36672662],
       [138.43805501, -35.36793484]])</code></pre>
<pre class="python"><code>plt.plot(faultsNeo[:,0],faultsNeo[:,1],&#39;.&#39;,markersize=0.1)
plt.plot(comm.LONGITUDE, comm.LATITUDE, marker=&#39;o&#39;, linestyle=&#39;&#39;, color=&#39;y&#39;)
plt.plot(xval,yval,&#39;grey&#39;,linewidth=0.5,label=&#39;SA&#39;)</code></pre>
<pre><code>[&lt;matplotlib.lines.Line2D at 0x22f4f0f9b88&gt;]</code></pre>
<div class="figure">
<img src="01-build_the_data_files/01-build_the_data_18_1.png" alt="" />
<p class="caption">png</p>
</div>
</div>
<div id="netcdf-formatted-raster-grids" class="section level3">
<h3>Netcdf formatted raster grids</h3>
<pre class="python"><code>#Define a function to read the netcdf files
def readnc(filename):
    tic=time.time()
    rasterfile=filename
    data = scipy.io.netcdf_file(rasterfile,&#39;r&#39;)
    xdata=data.variables[&#39;lon&#39;][:]
    ydata=data.variables[&#39;lat&#39;][:]
    zdata=np.array(data.variables[&#39;Band1&#39;][:])

    toc=time.time()
    print(&quot;Loaded&quot;, rasterfile, &quot;in&quot;, f&#39;{toc-tic:.2f}s&#39;)
    print(&quot;Spacing x&quot;, f&#39;{xdata[2]-xdata[1]:.2f}&#39;, &quot;y&quot;, f&#39;{ydata[2]-ydata[1]:.2f}&#39;, np.shape(zdata),np.min(xdata),np.max(xdata),np.min(ydata),f&#39;{np.max(ydata):.2f}&#39;)

    return(xdata,ydata,zdata)</code></pre>
<pre class="python"><code>#TODO: Should be cleaned up and put into dictionary or similar.
#For now, reading individual datasets is fine
x1,y1,z1 = readnc(&quot;data/sa-dem.nc&quot;)
x2,y2,z2 = readnc(&quot;data/sa-mag-tmi.nc&quot;)
x3,y3,z3 = readnc(&quot;data/sa-grav.nc&quot;)</code></pre>
<pre><code>Loaded data/sa-dem.nc in 0.02s
Spacing x 0.01 y 0.01 (1208, 1201) 129.005 141.005 -38.065 -25.99
Loaded data/sa-mag-tmi.nc in 0.01s
Spacing x 0.01 y 0.01 (1208, 1201) 129.005 141.005 -38.065 -25.99
Loaded data/sa-grav.nc in 0.01s
Spacing x 0.01 y 0.01 (1208, 1201) 129.005 141.005 -38.065 -25.99</code></pre>
<pre class="python"><code>plt.pcolormesh(x1,y1,z1,cmap=&#39;Greys&#39;,shading=&#39;auto&#39;)
plt.plot(comm.LONGITUDE, comm.LATITUDE, marker=&#39;o&#39;, linestyle=&#39;&#39;, color=&#39;y&#39;)
plt.plot(xval,yval,&#39;grey&#39;,linewidth=0.5,label=&#39;SA&#39;)</code></pre>
<pre><code>[&lt;matplotlib.lines.Line2D at 0x22f4f47af88&gt;]</code></pre>
<div class="figure">
<img src="01-build_the_data_files/01-build_the_data_22_1.png" alt="" />
<p class="caption">png</p>
</div>
</div>
<div id="categorical-geology-in-vector-polygons" class="section level3">
<h3>Categorical Geology in vector polygons</h3>
<pre class="python"><code>#Archean basement geology
geolshape=shapefile.Reader(&quot;data/Archaean_Early_Mesoprterzoic_polygons_shp/geology_archaean.shp&quot;)

recsArch   = geolshape.records()
shapesArch  = geolshape.shapes()</code></pre>
<pre class="python"><code>color = plt.cm.rainbow(np.linspace(0, 1, len(shapesArch)))
for i,c in enumerate(color): #, range(len(shapesArch)):
    boundary = shapesArch[i].points
    xs = [x for x, y in shapesArch[i].points]
    ys = [y for x, y in shapesArch[i].points]
    plt.fill(xs,ys,c=c)
    
plt.plot(comm.LONGITUDE, comm.LATITUDE, marker=&#39;o&#39;, linestyle=&#39;&#39;, color=&#39;y&#39;)
plt.plot(xval,yval,&#39;grey&#39;,linewidth=0.5,label=&#39;SA&#39;)</code></pre>
<pre><code>[&lt;matplotlib.lines.Line2D at 0x22f4f0dc388&gt;]</code></pre>
<div class="figure">
<img src="01-build_the_data_files/01-build_the_data_25_1.png" alt="" />
<p class="caption">png</p>
</div>
<pre class="python"><code></code></pre>
</div>
</div>
<div id="take-a-moment-to-appreciate-the-various-methods-you-have-used-just-to-load-the-data" class="section level2">
<h2>Take a moment to appreciate the various methods you have used just to load the data!</h2>
<p>Now, we need to assign the values of each of these geophyiscal datasets (predictor variables) to the target class (i.e. mineral deposit locations). The assumption being that the occurnece of some mineral deposit (e.g. Cu) is a function of x1, x2, x3, x4, x5, x6. Where the Resitivity is x1, the distance to a Neoprotezoic fault is x2, the value of DEM, magnetic TMI, and Gravity is x3, x4, and x5, and the geologica basement unit name is x6.</p>
</div>
</div>
<div id="part-2---spatial-data-mining-of-datasets" class="section level1">
<h1>Part 2 - Spatial data mining of datasets</h1>
<div id="select-the-commodity-and-geophysical-features-to-use" class="section level3">
<h3>Select the commodity and geophysical features to use</h3>
<p>Edit <em>commname</em> above and turn these labels on/off as required. Generally run data mining with all labels. Then can turn these features on/off before running ML if needed.</p>
<pre class="python"><code>lons=[&#39;lon&#39;,&#39;lat&#39;]
reslabels = [     
&#39;res-25&#39;,
&#39;res-77&#39;,
&#39;res-136&#39;,
&#39;res-201&#39;,
&#39;res-273&#39;,
&#39;res-353&#39;,
&#39;res-442&#39;,
&#39;res-541&#39;,
&#39;res-650&#39;,  
&#39;res-772&#39;,
&#39;res-907&#39;,
&#39;res-1056&#39;,
&#39;res-1223&#39;,
&#39;res-1407&#39;,
&#39;res-1612&#39;,
&#39;res-1839&#39;,
&#39;res-2092&#39;,
&#39;res-2372&#39;,
&#39;res-2683&#39;,
&#39;res-3028&#39;,
&#39;res-3411&#39;,
&#39;res-3837&#39;,    
&#39;res-4309&#39;,
&#39;res-4833&#39;,
&#39;res-5414&#39;,
&#39;res-6060&#39;,
&#39;res-6776&#39;,
&#39;res-7572&#39;,
&#39;res-8455&#39;,
&#39;res-9435&#39;,
&#39;res-10523&#39;,
&#39;res-11730&#39;,
&#39;res-13071&#39;,
&#39;res-14559&#39;,
&#39;res-16210&#39;,
&#39;res-18043&#39;,   
&#39;res-20078&#39;,
&#39;res-22337&#39;,
&#39;res-24844&#39;,
&#39;res-27627&#39;,
&#39;res-30716&#39;,
&#39;res-34145&#39;,
&#39;res-37951&#39;,
&#39;res-42175&#39;,
&#39;res-46865&#39;,
&#39;res-52070&#39;,
&#39;res-57847&#39;,
&#39;res-64261&#39;,
&#39;res-71379&#39;,
&#39;res-79281&#39;,
&#39;res-88052&#39;,
&#39;res-97788&#39;,
&#39;res-108595&#39;,
&#39;res-120590&#39;,
&#39;res-133905&#39;,
&#39;res-148685&#39;,
&#39;res-165090&#39;,
&#39;res-183300&#39;,
&#39;res-203513&#39;,
&#39;res-225950&#39;,
&#39;res-250854&#39;,
&#39;res-278498&#39;,
&#39;res-309183&#39;
]
  
faultlabels=[
    &quot;neoFaults&quot;,
    &quot;archFaults&quot;,
    &quot;gairFaults&quot;
]

numerical_features=reslabels+faultlabels+[
&quot;aster1-AlOH-cont&quot;,
&quot;aster2-AlOH&quot;,
&quot;aster3-FeOH-cont&quot;,
&quot;aster4-Ferric-cont&quot;,
&quot;aster5-Ferrous-cont&quot;,
&quot;aster6-Ferrous-index&quot;,
&quot;aster7-MgOH-comp&quot;,
&quot;aster8-MgOH-cont&quot;,
&quot;aster9-green&quot;,
&quot;aster10-kaolin&quot;,
&quot;aster11-opaque&quot;,
&quot;aster12-quartz&quot;,
&quot;aster13-regolith-b3&quot;,
&quot;aster14-regolith-b4&quot;,
&quot;aster15-silica&quot;,
&quot;base16&quot;,
&quot;dem17&quot;,
&quot;dtb18&quot;,
&quot;mag19-2vd&quot;,
&quot;mag20-rtp&quot;,
&quot;mag21-tmi&quot;,
&quot;rad22-dose&quot;,
&quot;rad23-k&quot;,
&quot;rad24-th&quot;,
&quot;rad25-u&quot;,
&quot;grav26&quot;
]

categorical_features=[
&#39;archean27&#39;,
&#39;geol28&#39;,
&#39;random&#39;
]</code></pre>
</div>
<div id="generate-the-non-deposit-dataset" class="section level3">
<h3>Generate the non-deposit dataset</h3>
<p>This step is important. There are numerous ways to generate our non-deposit set, each with different benefits and trade-offs. The randomisation of points throughout <em>some</em> domain appears to be robust. But you must think, is this domain a reasonable estimation of “background” geophysics/geology? Why are you picking these locations as non-deposits? Will they be over/under-representing actual deposits? Will they be over/under-representing actual non-deposits?</p>
<p>Change the lows, highs, and sizes as desired. And enforce the points are with some confinement area if needed. A good place to start is within the spatial extent of the known deposits/commodity.</p>
<pre class="python"><code>#Generate &quot;non-deposit&quot; points within the same spatial domains as deposits (e.g. on land, or in the gawler, or in SA).
#We may want to train and test just over the regions that the grids are valid.
#So we can crop the known deposits to the extent of the grids.

polgonshape=shapefile.Reader(&quot;SA-DATA/SA/SA_STATE_POLYGON_shp.shp&quot;)
#polgonshape=shapefile.Reader(&quot;SA-DATA/GCAS_Boundary/GCAS_Boundary.shp)
shapesPoly  = polgonshape.shapes()

#Now make a set of &quot;non-deposits&quot; using a random location within our exploration area
lats_rand=np.random.uniform(low=min(df.LATITUDE), high=max(df.LATITUDE), size=len(comm.LATITUDE))
lons_rand=np.random.uniform(low=min(df.LONGITUDE), high=max(df.LONGITUDE), size=len(comm.LONGITUDE))

#And enforce the random points are within our the shapefile boudary
#Probably more efficent ways to do this for larger datasets. Fine for now.
boundary=shapesPoly[1]
for i,_ in enumerate(lats_rand):
    while not Point((lons_rand[i],lats_rand[i])).within(shape(boundary)):
            lats_rand[i]=random.uniform(min(df.LATITUDE), max(df.LATITUDE))
            lons_rand[i]=random.uniform(min(df.LONGITUDE), max(df.LONGITUDE))
            
print(&quot;Produced&quot;, len(lats_rand),len(lons_rand), &quot;latitude-longitude pairs for non-deposits.&quot;)</code></pre>
<pre><code>Produced 115 115 latitude-longitude pairs for non-deposits.</code></pre>
<pre class="python"><code>#Save the SA polygon for plotting
xvalsa = [x[0] for x in shapesPoly[1].points]
yvalsa = [x[1] for x in shapesPoly[1].points]</code></pre>
<pre class="python"><code>#Quick plot of where commodity deposit data is and generated non-deposit data
ax = plt.axes(projection=ccrs.PlateCarree())
ax.coastlines()
ax.margins(0.05) # 5% padding to the map boundary so we can see the true extent nicely

ax.plot(comm.LONGITUDE, comm.LATITUDE, marker=&#39;o&#39;, linestyle=&#39;&#39;, color=&#39;y&#39;)
#ax.plot(lons_rand,lats_rand,marker=&#39;.&#39;,linestyle=&#39;&#39;,color=&#39;k&#39;)
plt.plot(xval,yval,label=&#39;Gawler&#39;)

plt.show()</code></pre>
<div class="figure">
<img src="01-build_the_data_files/01-build_the_data_33_0.png" alt="" />
<p class="caption">png</p>
</div>
</div>
<div id="define-function-which-performs-coregisteringdata-mining" class="section level3">
<h3>Define function which performs coregistering/data-mining</h3>
<pre class="python"><code>def coregLoop(sampleData):
    &#39;&#39;&#39;
    Define a function to coregister the grids and perform the spatial data mining.
    
    Requires list of lat and lon, will return the value at that point for all the hardcoded grids
    sampleData=[lat,lon]
    
    Returns array of parameters in the form:
    [lat, lon, param1, param2,...., param92,randomValue]
    
    TODO: Hadrcoded grids currently defined globally. Apply function to pass in grids for finer control.
    &#39;&#39;&#39;
    
    lat=sampleData[0]
    lon=sampleData[1]
    #Set the search space over which to sample the geophysical grid
    #Units are in same unit as source grids
    region=1 
    #Set the search sapce to sample the resitivity layers
    region2=100

    #Get the closest Resitivity indexes to the point
    idx,dist=coregPoint([lon,lat],lonlatres,region2)
    
    #Get the distance to the faults from the point
    _,dist=coregPoint([lon,lat],faultsNeo,region2)
    _,dist2=coregPoint([lon,lat],faultsArch,region2)
    _,dist3=coregPoint([lon,lat],faultsGair,region2)

    #Get the Numerical data indexes of the geophys at the point
    xloc1=(np.abs(np.array(x1) - lon).argmin())
    yloc1=(np.abs(np.array(y1) - lat).argmin())
    xloc2=(np.abs(np.array(x2) - lon).argmin())
    yloc2=(np.abs(np.array(y2) - lat).argmin())
    xloc3=(np.abs(np.array(x3) - lon).argmin())
    yloc3=(np.abs(np.array(y3) - lat).argmin())
    xloc4=(np.abs(np.array(x4) - lon).argmin())
    yloc4=(np.abs(np.array(y4) - lat).argmin())
    xloc5=(np.abs(np.array(x5) - lon).argmin())
    yloc5=(np.abs(np.array(y5) - lat).argmin())
    xloc6=(np.abs(np.array(x6) - lon).argmin())
    yloc6=(np.abs(np.array(y6) - lat).argmin())
    xloc7=(np.abs(np.array(x7) - lon).argmin())
    yloc7=(np.abs(np.array(y7) - lat).argmin())
    xloc8=(np.abs(np.array(x8) - lon).argmin())
    yloc8=(np.abs(np.array(y8) - lat).argmin())
    xloc9=(np.abs(np.array(x9) - lon).argmin())
    yloc9=(np.abs(np.array(y9) - lat).argmin())
    xloc10=(np.abs(np.array(x10) - lon).argmin())
    yloc10=(np.abs(np.array(y10) - lat).argmin())
    xloc11=(np.abs(np.array(x11) - lon).argmin())
    yloc11=(np.abs(np.array(y11) - lat).argmin())
    xloc12=(np.abs(np.array(x12) - lon).argmin())
    yloc12=(np.abs(np.array(y12) - lat).argmin())
    xloc13=(np.abs(np.array(x13) - lon).argmin())
    yloc13=(np.abs(np.array(y13) - lat).argmin())
    xloc14=(np.abs(np.array(x14) - lon).argmin())
    yloc14=(np.abs(np.array(y14) - lat).argmin())
    xloc15=(np.abs(np.array(x15) - lon).argmin())
    yloc15=(np.abs(np.array(y15) - lat).argmin())
    xloc16=(np.abs(np.array(x16) - lon).argmin())
    yloc16=(np.abs(np.array(y16) - lat).argmin())
    xloc17=(np.abs(np.array(x17) - lon).argmin())
    yloc17=(np.abs(np.array(y17) - lat).argmin())
    xloc18=(np.abs(np.array(x18) - lon).argmin())
    yloc18=(np.abs(np.array(y18) - lat).argmin())
    xloc19=(np.abs(np.array(x19) - lon).argmin())
    yloc19=(np.abs(np.array(y19) - lat).argmin())
    xloc20=(np.abs(np.array(x20) - lon).argmin())
    yloc20=(np.abs(np.array(y20) - lat).argmin())
    xloc21=(np.abs(np.array(x21) - lon).argmin())
    yloc21=(np.abs(np.array(y21) - lat).argmin())
    xloc22=(np.abs(np.array(x22) - lon).argmin())
    yloc22=(np.abs(np.array(y22) - lat).argmin())
    xloc23=(np.abs(np.array(x23) - lon).argmin())
    yloc23=(np.abs(np.array(y23) - lat).argmin())
    xloc24=(np.abs(np.array(x24) - lon).argmin())
    yloc24=(np.abs(np.array(y24) - lat).argmin())
    xloc25=(np.abs(np.array(x25) - lon).argmin())
    yloc25=(np.abs(np.array(y25) - lat).argmin())
    xloc26=(np.abs(np.array(x26) - lon).argmin())
    yloc26=(np.abs(np.array(y26) - lat).argmin())

    
    #Numerical data values
    z1val=coregRaster([xloc1,yloc1],z1,region)
    z2val=coregRaster([xloc2,yloc2],z2,region)
    z3val=coregRaster([xloc3,yloc3],z3,region)
    z4val=coregRaster([xloc4,yloc4],z4,region)
    z5val=coregRaster([xloc5,yloc5],z5,region)
    z6val=coregRaster([xloc6,yloc6],z6,region)
    z7val=coregRaster([xloc7,yloc7],z7,region)
    z8val=coregRaster([xloc8,yloc8],z8,region)
    z9val=coregRaster([xloc9,yloc9],z9,region)
    z10val=coregRaster([xloc10,yloc10],z10,region)
    z11val=coregRaster([xloc11,yloc11],z11,region)
    z12val=coregRaster([xloc12,yloc12],z12,region)
    z13val=coregRaster([xloc13,yloc13],z13,region)
    z14val=coregRaster([xloc14,yloc14],z14,region)
    z15val=coregRaster([xloc15,yloc15],z15,region)
    z16val=coregRaster([xloc16,yloc16],z16,region)
    z17val=coregRaster([xloc17,yloc17],z17,region)
    z18val=coregRaster([xloc18,yloc18],z18,region)
    z19val=coregRaster([xloc19,yloc19],z19,region)
    z20val=coregRaster([xloc20,yloc20],z20,region)
    z21val=coregRaster([xloc21,yloc21],z21,region)
    z22val=coregRaster([xloc22,yloc22],z22,region)
    z23val=coregRaster([xloc23,yloc23],z23,region)
    z24val=coregRaster([xloc24,yloc24],z24,region)
    z25val=coregRaster([xloc25,yloc25],z25,region)
    z26val=coregRaster([xloc26,yloc26],z26,region)
    
    #Append all the values to an array to return
    #Return dummys for categorical data for now
    vals=np.array([lon,lat])
    vals=np.append(vals,f[:,idx,3])
    vals=np.append(vals,
                   [
                    dist,dist2,dist3,
                    z1val,z2val,z3val,
                    z4val,z5val,z6val,
                    z7val,z8val,z9val,
                        z10val,z11val,z12val,
                    z13val,z14val,z15val,
                    z16val,z17val,z18val,
                    z19val,z20val,z21val,
                    z22val,z23val,z24val,
                    z25val,z26val,
                    -9999.,-9999.
                   ])
    #Append a random choice of 999 or -999 to benchmark ML
    coregMap=np.append(vals,[random.choice([-999, 999])])
    
    #Return the data
    return(coregMap)
    
</code></pre>
</div>
<div id="run-spatial-mining-of-known-deposits-and-non-deposits" class="section level2">
<h2>Run spatial mining of known deposits and “non-deposits”</h2>
<p>Must be re-run on each commodity change. Can be saved and just loaded in if data has already been generated.</p>
<pre class="python"><code>#Load in co-registerd training data
training_data=pd.read_csv(&quot;ML-DATA/training_data-&quot;+commname+&quot;.csv&quot;,header=0)

#Or if that does not exist run next two cells....</code></pre>
<pre class="python"><code>#Interrogate the data associated with deposits

#Coregloop takes about 0.07s per call
#shapeExplore takes about 1.7s per call

tic=time.time()
deps1=[]
for row in comm.itertuples():
    lazy_result = coregLoop([row.LATITUDE,row.LONGITUDE])
    #lazy_result = delayed(coregLoop)([row.LATITUDE,row.LONGITUDE])
    deps1.append(lazy_result)
    
vec1=pd.DataFrame(np.squeeze(deps1),columns=lons+numerical_features+categorical_features)
vec1[&#39;deposit&#39;] = 1 #Add the &quot;depoist category flag&quot;

toc=time.time()
print(&quot;Time deposits:&quot;, toc-tic, &quot; seconds&quot;)
tic=time.time()

#Interrogate the data associated with randomly smapled points to use as counter-examples
deps0=[]
for lat,lon in zip(lats_rand,lons_rand):
    lazy_result = coregLoop([lat,lon])
    #lazy_result = delayed(coregLoop)([lat,lon])
    deps0.append(lazy_result)
    
vec2=pd.DataFrame(np.squeeze(deps0),columns=lons+numerical_features+categorical_features)
vec2[&#39;deposit&#39;] = 0 #Add the &quot;non-deposit category flag&quot;

toc=time.time()
print(&quot;Time non-deposits:&quot;, toc-tic, &quot; seconds&quot;)


#Combine the datasets
training_data = pd.concat([vec1, vec2], ignore_index=True)

tic=time.time()

#Add the categorical shapefile data
training_data[&#39;geol28&#39;]=training_data.apply(shapeExplore, args=(shapesGeol,recsGeol,1), axis=1)
training_data[&#39;archean27&#39;]=training_data.apply(shapeExplore, args=(shapesArch,recsArch,-1), axis=1)

toc=time.time()
print(&quot;Time geology:&quot;, toc-tic, &quot; seconds&quot;)</code></pre>
<pre><code>/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:54: RuntimeWarning: Mean of empty slice


Time deposits: 15.536046981811523  seconds
Time non-deposits: 15.219344139099121  seconds
Time geology: 40.391884565353394  seconds</code></pre>
<pre class="python"><code>#And save the training data out to a file
training_data.to_csv(&quot;ML-DATA/training_data-&quot;+commname+&quot;.csv&quot;)</code></pre>
<pre class="python"><code>#Use this to clean rows from ML if particular data does not have exisitng data and would dilute the models too mcuh
# training_data[&#39;badsum&#39;]=(training_data == -9999.).astype(int).sum(axis=1)
# (training_data == -9999).astype(int).sum(axis=1).value_counts()
# #If many of the points have no data, drop them
# indexNames = training_data[ training_data[&#39;badsum&#39;] &gt; 10 ].index
# training_data.drop(indexNames, inplace=True)
# #training_data.drop(columns=[&#39;badsum&#39;], inplace=True)
# #indexNames = training_data[ training_data[&#39;17dem&#39;] == 0 ].index
# #training_data.drop(indexNames , inplace=True)
# #Save number of deps/non-deps
# lennon=len(training_data.deposit[training_data.deposit==0])
# lendep=len(training_data.deposit[training_data.deposit==1])

# training_data
</code></pre>
<pre class="python"><code>#Save number of deps/non-deps, to be used for counting later
lennon=len(training_data.deposit[training_data.deposit==0])
lendep=len(training_data.deposit[training_data.deposit==1])
print(lennon,lendep)

#And look at the coregistered training data we have data-mined!
training_data</code></pre>
<pre><code>115 115</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th>
</th>
<th>
lon
</th>
<th>
lat
</th>
<th>
res-25
</th>
<th>
res-77
</th>
<th>
res-136
</th>
<th>
res-201
</th>
<th>
res-273
</th>
<th>
res-353
</th>
<th>
res-442
</th>
<th>
res-541
</th>
<th>
…
</th>
<th>
mag21-tmi
</th>
<th>
rad22-dose
</th>
<th>
rad23-k
</th>
<th>
rad24-th
</th>
<th>
rad25-u
</th>
<th>
grav26
</th>
<th>
archean27
</th>
<th>
geol28
</th>
<th>
random
</th>
<th>
deposit
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
0
</th>
<td>
139.179436
</td>
<td>
-29.877637
</td>
<td>
2.2135
</td>
<td>
2.2916
</td>
<td>
2.3263
</td>
<td>
2.3432
</td>
<td>
2.3547
</td>
<td>
2.3644
</td>
<td>
2.3725
</td>
<td>
2.3779
</td>
<td>
…
</td>
<td>
-103.815964
</td>
<td>
53.314350
</td>
<td>
1.326607
</td>
<td>
13.133085
</td>
<td>
77.592484
</td>
<td>
1.656689
</td>
<td>
19214
</td>
<td>
8602
</td>
<td>
999.0
</td>
<td>
1
</td>
</tr>
<tr>
<th>
1
</th>
<td>
138.808767
</td>
<td>
-30.086296
</td>
<td>
2.3643
</td>
<td>
2.4819
</td>
<td>
2.5282
</td>
<td>
2.5482
</td>
<td>
2.5608
</td>
<td>
2.5713
</td>
<td>
2.5803
</td>
<td>
2.5875
</td>
<td>
…
</td>
<td>
-203.493454
</td>
<td>
57.424988
</td>
<td>
1.505081
</td>
<td>
10.301062
</td>
<td>
65.972046
</td>
<td>
-12.404812
</td>
<td>
19218
</td>
<td>
6658
</td>
<td>
999.0
</td>
<td>
1
</td>
</tr>
<tr>
<th>
2
</th>
<td>
138.752281
</td>
<td>
-30.445684
</td>
<td>
2.1141
</td>
<td>
2.1535
</td>
<td>
2.1710
</td>
<td>
2.1804
</td>
<td>
2.1879
</td>
<td>
2.1954
</td>
<td>
2.2035
</td>
<td>
2.2124
</td>
<td>
…
</td>
<td>
-167.319275
</td>
<td>
94.660133
</td>
<td>
2.568490
</td>
<td>
20.143124
</td>
<td>
85.115166
</td>
<td>
-6.649049
</td>
<td>
19218
</td>
<td>
15524
</td>
<td>
999.0
</td>
<td>
1
</td>
</tr>
<tr>
<th>
3
</th>
<td>
138.530506
</td>
<td>
-30.533225
</td>
<td>
2.2234
</td>
<td>
2.3151
</td>
<td>
2.3644
</td>
<td>
2.3946
</td>
<td>
2.4182
</td>
<td>
2.4402
</td>
<td>
2.4621
</td>
<td>
2.4846
</td>
<td>
…
</td>
<td>
-131.731750
</td>
<td>
73.580902
</td>
<td>
2.218921
</td>
<td>
14.252978
</td>
<td>
68.107780
</td>
<td>
-11.395281
</td>
<td>
19218
</td>
<td>
15524
</td>
<td>
-999.0
</td>
<td>
1
</td>
</tr>
<tr>
<th>
4
</th>
<td>
138.887019
</td>
<td>
-30.565479
</td>
<td>
2.1982
</td>
<td>
2.2647
</td>
<td>
2.2899
</td>
<td>
2.2987
</td>
<td>
2.3022
</td>
<td>
2.3038
</td>
<td>
2.3039
</td>
<td>
2.3022
</td>
<td>
…
</td>
<td>
-194.104736
</td>
<td>
70.939156
</td>
<td>
1.907848
</td>
<td>
10.630255
</td>
<td>
73.486282
</td>
<td>
-1.273882
</td>
<td>
19218
</td>
<td>
13222
</td>
<td>
-999.0
</td>
<td>
1
</td>
</tr>
<tr>
<th>
…
</th>
<td>
…
</td>
<td>
…
</td>
<td>
…
</td>
<td>
…
</td>
<td>
…
</td>
<td>
…
</td>
<td>
…
</td>
<td>
…
</td>
<td>
…
</td>
<td>
…
</td>
<td>
…
</td>
<td>
…
</td>
<td>
…
</td>
<td>
…
</td>
<td>
…
</td>
<td>
…
</td>
<td>
…
</td>
<td>
…
</td>
<td>
…
</td>
<td>
…
</td>
<td>
…
</td>
</tr>
<tr>
<th>
225
</th>
<td>
140.950913
</td>
<td>
-36.818736
</td>
<td>
2.0303
</td>
<td>
2.0433
</td>
<td>
2.0518
</td>
<td>
2.0586
</td>
<td>
2.0650
</td>
<td>
2.0716
</td>
<td>
2.0787
</td>
<td>
2.0865
</td>
<td>
…
</td>
<td>
64.354500
</td>
<td>
31.400856
</td>
<td>
0.577390
</td>
<td>
6.478987
</td>
<td>
53.581688
</td>
<td>
-12.427198
</td>
<td>
19222
</td>
<td>
5028
</td>
<td>
-999.0
</td>
<td>
0
</td>
</tr>
<tr>
<th>
226
</th>
<td>
139.584019
</td>
<td>
-33.121195
</td>
<td>
2.0452
</td>
<td>
2.0671
</td>
<td>
2.0715
</td>
<td>
2.0635
</td>
<td>
2.0485
</td>
<td>
2.0289
</td>
<td>
2.0052
</td>
<td>
1.9775
</td>
<td>
…
</td>
<td>
-613.181885
</td>
<td>
52.888771
</td>
<td>
1.724605
</td>
<td>
9.917326
</td>
<td>
48.059807
</td>
<td>
-11.312914
</td>
<td>
19218
</td>
<td>
15524
</td>
<td>
-999.0
</td>
<td>
0
</td>
</tr>
<tr>
<th>
227
</th>
<td>
134.221487
</td>
<td>
-28.522637
</td>
<td>
2.0157
</td>
<td>
2.0031
</td>
<td>
1.9811
</td>
<td>
1.9584
</td>
<td>
1.9370
</td>
<td>
1.9161
</td>
<td>
1.8942
</td>
<td>
1.8699
</td>
<td>
…
</td>
<td>
123.653641
</td>
<td>
55.559757
</td>
<td>
0.828879
</td>
<td>
15.572678
</td>
<td>
62.832291
</td>
<td>
-24.688011
</td>
<td>
18446
</td>
<td>
6658
</td>
<td>
999.0
</td>
<td>
0
</td>
</tr>
<tr>
<th>
228
</th>
<td>
131.427746
</td>
<td>
-30.738339
</td>
<td>
1.9768
</td>
<td>
1.9636
</td>
<td>
1.9494
</td>
<td>
1.9326
</td>
<td>
1.9126
</td>
<td>
1.8891
</td>
<td>
1.8617
</td>
<td>
1.8299
</td>
<td>
…
</td>
<td>
114.935860
</td>
<td>
34.257481
</td>
<td>
0.830566
</td>
<td>
6.841074
</td>
<td>
50.776741
</td>
<td>
-15.348145
</td>
<td>
18012
</td>
<td>
6684
</td>
<td>
-999.0
</td>
<td>
0
</td>
</tr>
<tr>
<th>
229
</th>
<td>
133.551967
</td>
<td>
-31.226474
</td>
<td>
2.2125
</td>
<td>
2.2646
</td>
<td>
2.2777
</td>
<td>
2.2819
</td>
<td>
2.2856
</td>
<td>
2.2897
</td>
<td>
2.2929
</td>
<td>
2.2932
</td>
<td>
…
</td>
<td>
22.312160
</td>
<td>
10.491249
</td>
<td>
0.123158
</td>
<td>
2.786360
</td>
<td>
35.205452
</td>
<td>
-31.615749
</td>
<td>
18294
</td>
<td>
16926
</td>
<td>
999.0
</td>
<td>
0
</td>
</tr>
</tbody>
</table>
<p>
230 rows × 98 columns
</p>
</div>
</div>
<div id="run-spatial-mining-of-gawler-target-data" class="section level2">
<h2>Run spatial mining of gawler target data</h2>
<p>Only needs to be done once. Each commodity uses this same dataset for targetting. The values of the grid are used to predict whatever commodity is run. Depending on target resolution and whether using parallel versions, can take a good amount of time.</p>
<pre class="python"><code>#Load in target data
target_data=pd.read_csv(&quot;ML-DATA/target_data.csv&quot;,header=0)

#OR run the next 5 cells....</code></pre>
<pre class="python"><code>################ RUN FROM HERE ONCE (or use the HPC versions for high-res) ##########################
#Make a regularly spaced grid here for use in making a probablilty map later
lats_reg=np.linspace(min(yval),max(yval),10)
lons_reg=np.linspace(min(xval),max(xval),10)
#lats_reg=np.arange(min(yval),max(yval)+0.0100,0.0100)
#lons_reg=np.arange(min(xval),max(xval)+0.0100,0.0100)

sampleData=[]
for lat in lats_reg:
    for lon in lons_reg:
            sampleData.append([lat, lon])
            
print(np.size(sampleData))</code></pre>
<pre class="python"><code>#Run the data-mining/coregistration
gridgawler=[]
tic=time.time()
for geophysparams in sampleData:
    #lazy_result = delayed(coregLoop)(geophysparams)
    lazy_result = coregLoop(geophysparams)
    gridgawler.append(lazy_result)
print(&quot;appended, now running...&quot;)

#c=compute(gridgawler)
toc=time.time()

print(&quot;Time taken:&quot;, toc-tic, &quot; seconds&quot;)</code></pre>
<pre class="python"><code>#Clean up the output file
target_data=pd.DataFrame(np.squeeze(gridgawler),columns=lons+numerical_features+categorical_features)</code></pre>
<pre class="python"><code>#Add the categorical shapefile data
target_data[&#39;geol28&#39;]=target_data.apply(shapeExplore, args=(shapesGeol,recsGeol,1), axis=1)
target_data[&#39;archean27&#39;]=target_data.apply(shapeExplore, args=(shapesArch,recsArch,-1), axis=1)</code></pre>
<pre class="python"><code>#Save out the data, and no need to run the co-registration again.
target_data.to_csv(&quot;target_data.csv&quot;,index=False)

################## RUN TO HERE ONCE #########################</code></pre>
<pre class="python"><code>#Look at the target data
#Should be in the same form as the training data WITHOUT the information of whether it is a deposit or non-deposit.
target_data</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th>
</th>
<th>
lon
</th>
<th>
lat
</th>
<th>
res-25
</th>
<th>
res-77
</th>
<th>
res-136
</th>
<th>
res-201
</th>
<th>
res-273
</th>
<th>
res-353
</th>
<th>
res-442
</th>
<th>
res-541
</th>
<th>
…
</th>
<th>
mag20-rtp
</th>
<th>
mag21-tmi
</th>
<th>
rad22-dose
</th>
<th>
rad23-k
</th>
<th>
rad24-th
</th>
<th>
rad25-u
</th>
<th>
grav26
</th>
<th>
archean27
</th>
<th>
geol28
</th>
<th>
random
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
0
</th>
<td>
131.000009
</td>
<td>
-32.664987
</td>
<td>
-0.8814
</td>
<td>
-0.8562
</td>
<td>
-0.7231
</td>
<td>
-0.5389
</td>
<td>
-0.3426
</td>
<td>
-0.1523
</td>
<td>
0.0268
</td>
<td>
0.1949
</td>
<td>
…
</td>
<td>
-9999.000000
</td>
<td>
-9999.000000
</td>
<td>
-9999.0
</td>
<td>
-9999.0
</td>
<td>
-9999.0
</td>
<td>
-9999.0
</td>
<td>
-9999.000000
</td>
<td>
14536
</td>
<td>
1002
</td>
<td>
999.0
</td>
</tr>
<tr>
<th>
1
</th>
<td>
131.010009
</td>
<td>
-32.664987
</td>
<td>
-0.8814
</td>
<td>
-0.8562
</td>
<td>
-0.7231
</td>
<td>
-0.5389
</td>
<td>
-0.3426
</td>
<td>
-0.1523
</td>
<td>
0.0268
</td>
<td>
0.1949
</td>
<td>
…
</td>
<td>
-9999.000000
</td>
<td>
-9999.000000
</td>
<td>
-9999.0
</td>
<td>
-9999.0
</td>
<td>
-9999.0
</td>
<td>
-9999.0
</td>
<td>
-9999.000000
</td>
<td>
14536
</td>
<td>
1002
</td>
<td>
999.0
</td>
</tr>
<tr>
<th>
2
</th>
<td>
131.020009
</td>
<td>
-32.664987
</td>
<td>
-0.8814
</td>
<td>
-0.8562
</td>
<td>
-0.7231
</td>
<td>
-0.5389
</td>
<td>
-0.3426
</td>
<td>
-0.1523
</td>
<td>
0.0268
</td>
<td>
0.1949
</td>
<td>
…
</td>
<td>
-9999.000000
</td>
<td>
-9999.000000
</td>
<td>
-9999.0
</td>
<td>
-9999.0
</td>
<td>
-9999.0
</td>
<td>
-9999.0
</td>
<td>
-9999.000000
</td>
<td>
14536
</td>
<td>
1002
</td>
<td>
-999.0
</td>
</tr>
<tr>
<th>
3
</th>
<td>
131.030009
</td>
<td>
-32.664987
</td>
<td>
-0.8814
</td>
<td>
-0.8562
</td>
<td>
-0.7231
</td>
<td>
-0.5389
</td>
<td>
-0.3426
</td>
<td>
-0.1523
</td>
<td>
0.0268
</td>
<td>
0.1949
</td>
<td>
…
</td>
<td>
-9999.000000
</td>
<td>
-9999.000000
</td>
<td>
-9999.0
</td>
<td>
-9999.0
</td>
<td>
-9999.0
</td>
<td>
-9999.0
</td>
<td>
-9999.000000
</td>
<td>
14536
</td>
<td>
1002
</td>
<td>
999.0
</td>
</tr>
<tr>
<th>
4
</th>
<td>
131.040009
</td>
<td>
-32.664987
</td>
<td>
-0.8814
</td>
<td>
-0.8562
</td>
<td>
-0.7231
</td>
<td>
-0.5389
</td>
<td>
-0.3426
</td>
<td>
-0.1523
</td>
<td>
0.0268
</td>
<td>
0.1949
</td>
<td>
…
</td>
<td>
-9999.000000
</td>
<td>
-9999.000000
</td>
<td>
-9999.0
</td>
<td>
-9999.0
</td>
<td>
-9999.0
</td>
<td>
-9999.0
</td>
<td>
-9999.000000
</td>
<td>
14536
</td>
<td>
1002
</td>
<td>
999.0
</td>
</tr>
<tr>
<th>
…
</th>
<td>
…
</td>
<td>
…
</td>
<td>
…
</td>
<td>
…
</td>
<td>
…
</td>
<td>
…
</td>
<td>
…
</td>
<td>
…
</td>
<td>
…
</td>
<td>
…
</td>
<td>
…
</td>
<td>
…
</td>
<td>
…
</td>
<td>
…
</td>
<td>
…
</td>
<td>
…
</td>
<td>
…
</td>
<td>
…
</td>
<td>
…
</td>
<td>
…
</td>
<td>
…
</td>
</tr>
<tr>
<th>
374161
</th>
<td>
137.970009
</td>
<td>
-27.344987
</td>
<td>
1.9917
</td>
<td>
1.9870
</td>
<td>
1.9831
</td>
<td>
1.9793
</td>
<td>
1.9749
</td>
<td>
1.9697
</td>
<td>
1.9647
</td>
<td>
1.9629
</td>
<td>
…
</td>
<td>
60.714424
</td>
<td>
77.071075
</td>
<td>
-9999.0
</td>
<td>
-9999.0
</td>
<td>
-9999.0
</td>
<td>
-9999.0
</td>
<td>
-36.425686
</td>
<td>
14548
</td>
<td>
2030
</td>
<td>
-999.0
</td>
</tr>
<tr>
<th>
374162
</th>
<td>
137.980009
</td>
<td>
-27.344987
</td>
<td>
1.9917
</td>
<td>
1.9870
</td>
<td>
1.9831
</td>
<td>
1.9793
</td>
<td>
1.9749
</td>
<td>
1.9697
</td>
<td>
1.9647
</td>
<td>
1.9629
</td>
<td>
…
</td>
<td>
58.152508
</td>
<td>
72.153748
</td>
<td>
-9999.0
</td>
<td>
-9999.0
</td>
<td>
-9999.0
</td>
<td>
-9999.0
</td>
<td>
-35.383732
</td>
<td>
14548
</td>
<td>
2030
</td>
<td>
-999.0
</td>
</tr>
<tr>
<th>
374163
</th>
<td>
137.990009
</td>
<td>
-27.344987
</td>
<td>
1.9924
</td>
<td>
1.9882
</td>
<td>
1.9849
</td>
<td>
1.9819
</td>
<td>
1.9785
</td>
<td>
1.9746
</td>
<td>
1.9707
</td>
<td>
1.9681
</td>
<td>
…
</td>
<td>
56.024078
</td>
<td>
66.960632
</td>
<td>
-9999.0
</td>
<td>
-9999.0
</td>
<td>
-9999.0
</td>
<td>
-9999.0
</td>
<td>
-34.756775
</td>
<td>
14548
</td>
<td>
2030
</td>
<td>
999.0
</td>
</tr>
<tr>
<th>
374164
</th>
<td>
138.000009
</td>
<td>
-27.344987
</td>
<td>
1.9924
</td>
<td>
1.9882
</td>
<td>
1.9849
</td>
<td>
1.9819
</td>
<td>
1.9785
</td>
<td>
1.9746
</td>
<td>
1.9707
</td>
<td>
1.9681
</td>
<td>
…
</td>
<td>
53.722004
</td>
<td>
61.110107
</td>
<td>
-9999.0
</td>
<td>
-9999.0
</td>
<td>
-9999.0
</td>
<td>
-9999.0
</td>
<td>
-34.204567
</td>
<td>
14548
</td>
<td>
2030
</td>
<td>
999.0
</td>
</tr>
<tr>
<th>
374165
</th>
<td>
138.010009
</td>
<td>
-27.344987
</td>
<td>
1.9924
</td>
<td>
1.9882
</td>
<td>
1.9849
</td>
<td>
1.9819
</td>
<td>
1.9785
</td>
<td>
1.9746
</td>
<td>
1.9707
</td>
<td>
1.9681
</td>
<td>
…
</td>
<td>
51.256954
</td>
<td>
54.969093
</td>
<td>
-9999.0
</td>
<td>
-9999.0
</td>
<td>
-9999.0
</td>
<td>
-9999.0
</td>
<td>
-33.568863
</td>
<td>
14548
</td>
<td>
2030
</td>
<td>
999.0
</td>
</tr>
</tbody>
</table>
<p>
374166 rows × 97 columns
</p>
</div>
</div>
</div>
<div id="part-3---machine-learning-model" class="section level1">
<h1>Part 3 - Machine learning model</h1>
<p>Now we have a fully data-mined, coregistered set of geophysical features. Both for training with known information about our deposits classification labels, and also a target set, we can build and apply the Machine Learning model classifier.</p>
<pre class="python"><code>#First, we can check some details about the data. 
#Simple check whether at least MOST of the geophysical parameters have a reasonable value associated with them.
[print(training_data.columns[i],j) for i,j in enumerate(training_data.median())]
#If any of these score -9999.0 it is recommended to remove from that column from analysis
#You can do this, by now &quot;commenting out&quot; the layer in cell 27.</code></pre>
<pre><code>lon 138.24976824878848
lat -31.4234139
res-25 2.0562500000000004
res-77 2.07275
res-136 2.0737
res-201 2.0729
res-273 2.0686
res-353 2.0707
res-442 2.06045
res-541 2.0584
res-650 2.0572999999999997
res-772 2.0549999999999997
res-907 2.0544000000000002
res-1056 2.0513
res-1223 2.05735
res-1407 2.0737
res-1612 2.0917000000000003
res-1839 2.0865
res-2092 2.0808999999999997
res-2372 2.0644
res-2683 2.01925
res-3028 2.0089
res-3411 2.0092999999999996
res-3837 1.9667
res-4309 1.98135
res-4833 1.9901499999999999
res-5414 2.0079000000000002
res-6060 2.1113999999999997
res-6776 2.1632
res-7572 2.252
res-8455 2.3027499999999996
res-9435 2.35195
res-10523 2.4196
res-11730 2.4535
res-13071 2.4711499999999997
res-14559 2.5052
res-16210 2.4838
res-18043 2.4543999999999997
res-20078 2.41535
res-22337 2.3192500000000003
res-24844 2.3199
res-27627 2.33305
res-30716 2.2951
res-34145 2.3369999999999997
res-37951 2.372
res-42175 2.3846499999999997
res-46865 2.4406499999999998
res-52070 2.4587
res-57847 2.45555
res-64261 2.51445
res-71379 2.6096500000000002
res-79281 2.6209
res-88052 2.66555
res-97788 2.6927000000000003
res-108595 2.7203999999999997
res-120590 2.68045
res-133905 2.6466000000000003
res-148685 2.5894500000000003
res-165090 2.5436
res-183300 2.4987500000000002
res-203513 2.4674500000000004
res-225950 2.3987
res-250854 2.3303000000000003
res-278498 2.2708
res-309183 2.1997999999999998
neoFaults 0.0900854008002272
archFaults 0.29892370502523613
gairFaults 1.2047528939475927
aster1-AlOH-cont 1.9861243963241577
aster2-AlOH 1.0791621804237366
aster3-FeOH-cont 1.9664206504821777
aster4-Ferric-cont 1.3717382550239563
aster5-Ferrous-cont 0.788004457950592
aster6-Ferrous-index 0.8526997566223145
aster7-MgOH-comp 0.9859207570552826
aster8-MgOH-cont 1.0112017393112183
aster9-green 1.4036157727241516
aster10-kaolin 0.9091927707195282
aster11-opaque -9999.0
aster12-quartz 0.5054315328598022
aster13-regolith-b3 0.9444536566734314
aster14-regolith-b4 1.2997069358825684
aster15-silica 1.066483736038208
base16 158.21937561035156
dem17 218.34683990478516
dtb18 -9999.0
mag19-2vd -1.4414607676371816e-06
mag20-rtp -111.75115966796875
mag21-tmi -117.75409317016602
rad22-dose 40.579397201538086
rad23-k 0.9994525611400604
rad24-th 8.156786918640137
rad25-u 51.71901512145996
grav26 -17.332341194152832
archean27 19208.0
geol28 15514.0
random 999.0
deposit 0.5





[None,
 None,
 None,
 None,
 None,
 None,
 None,
 None,
 None,
 None,
 None,
 None,
 None,
 None,
 None,
 None,
 None,
 None,
 None,
 None,
 None,
 None,
 None,
 None,
 None,
 None,
 None,
 None,
 None,
 None,
 None,
 None,
 None,
 None,
 None,
 None,
 None,
 None,
 None,
 None,
 None,
 None,
 None,
 None,
 None,
 None,
 None,
 None,
 None,
 None,
 None,
 None,
 None,
 None,
 None,
 None,
 None,
 None,
 None,
 None,
 None,
 None,
 None,
 None,
 None,
 None,
 None,
 None,
 None,
 None,
 None,
 None,
 None,
 None,
 None,
 None,
 None,
 None,
 None,
 None,
 None,
 None,
 None,
 None,
 None,
 None,
 None,
 None,
 None,
 None,
 None,
 None,
 None,
 None,
 None,
 None,
 None,
 None]</code></pre>
<div id="ml-classification" class="section level3">
<h3>ML Classification</h3>
<p>This is where the ML classifier is defined. We can substitue our favourite ML technique here, and tune model variables as desired. The default choices are recommended for the Gawler region.</p>
<pre class="python"><code>#Create the &#39;feature vector&#39; and a &#39;target classification vector&#39;
features=training_data[numerical_features+categorical_features]
targets=training_data.deposit

#Create the ML classifier with numerical and categorical data
#Scale, and replace missing values
numeric_transformer = Pipeline(steps=[
    (&#39;imputer&#39;,SimpleImputer(missing_values=-9999., strategy=&#39;median&#39;)),
    (&#39;scaler&#39;, StandardScaler())])

#Encode categorical data and fill missing values with default 0
categorical_transformer = Pipeline(steps=[
    (&#39;imputer&#39;, SimpleImputer(strategy=&#39;constant&#39;)),
    (&#39;onehot&#39;, OneHotEncoder(handle_unknown=&#39;ignore&#39;))])

#Combine numerical and categorical data
preprocessor = ColumnTransformer(transformers=[
        (&#39;num&#39;, numeric_transformer, numerical_features),
        (&#39;cat&#39;, categorical_transformer, categorical_features)])

# Append classifier to preprocessing pipeline.
# Now we have a full prediction pipeline.
rf = Pipeline(steps=[(&#39;preprocessor&#39;, preprocessor),
                (&#39;classifier&#39;, RandomForestClassifier(random_state=1))])
</code></pre>
<pre class="python"><code>#You can apply weighting to the model here. 
#We find manual selction (i.e with sound geological reasoning) of deposits is probably more robust 
#than applying arbitrary weighting of class labels. 
#Nevertheless, we can do this if desired by uncommenting and tweaking  the following.

weights=np.ones(len(training_data))
weightcount=0

#Algorithm for setting weight values, point by point
# for i,row in enumerate(training_data[training_data.deposit==1].itertuples()):
#     xloc1=(np.abs(np.array(comm.LONGITUDE) - row.lon).argmin())
#     if comm.loc[xloc1].SIZE_VAL==&quot;Locally Significant&quot;:
#         weights[i]=2
#     elif comm.loc[xloc1].SIZE_VAL==&quot;Significant to SA&quot;:
#         weights[i]=4
#     elif comm.loc[xloc1].SIZE_VAL==&quot;Significant to Australia&quot;:
#         weights[i]=8
#     elif comm.loc[xloc1].SIZE_VAL==&quot;World-wide Significance&quot;:
#         weights[i]=16
#     else:
#         #Else keep the weight at 1
#         weightcount+=1
#         weights[i]=0
#         continue
        

    </code></pre>
<pre class="python"><code>print(&#39;Tranining the Clasifier...&#39;)
rf.fit(features,targets,**{&#39;classifier__sample_weight&#39;: weights})

print(&quot;Done RF. Now scoring...&quot;)
scores = cross_val_score(rf, features,targets, cv=10)

print(&quot;RF 10-fold cross validation Scores:&quot;, scores)
print(&quot;SCORE Mean: %.2f&quot; % np.mean(scores), &quot;STD: %.2f&quot; % np.std(scores), &quot;\n&quot;)

plt.plot(targets.values,&#39;b-&#39;,label=&#39;Target (expected)&#39;)
plt.plot(rf.predict(features),&#39;rx&#39;,label=&#39;Prediction&#39;)
plt.xlabel(&quot;Feature set&quot;)
plt.ylabel(&quot;Target/Prediction&quot;)
plt.legend(loc=7)</code></pre>
<pre><code>Tranining the Clasifier...
Done RF. Now scoring...
RF 10-fold cross validation Scores: [0.95652174 0.86956522 1.         0.86956522 0.91304348 0.91304348
 0.7826087  0.86956522 0.82608696 0.69565217]
SCORE Mean: 0.87 STD: 0.08 






&lt;matplotlib.legend.Legend at 0x7ff04942bd30&gt;</code></pre>
<div class="figure">
<img src="01-build_the_data_files/01-build_the_data_55_2.png" alt="" />
<p class="caption">png</p>
</div>
<pre class="python"><code>#Make a plot out the feature scores. 
#These are the important parameters that are correlated with the deposits.

ft_idx=[]
ft_lab=[]
all_idx=[]
all_lab=[]
all_dat=[]
#Just print the significant features above some threshold
for i,lab in enumerate(np.append(numerical_features,rf[&#39;preprocessor&#39;].transformers_[1][1][&#39;onehot&#39;].get_feature_names(categorical_features))):
    all_dat.append([i,lab,rf.steps[1][1].feature_importances_[i]])
    all_lab.append(lab)
    all_idx.append(i)
    if rf.steps[1][1].feature_importances_[i] &gt;1*np.median(rf.steps[1][1].feature_importances_): 
        ft_idx.append(i)
        ft_lab.append(lab)
        </code></pre>
<pre class="python"><code>#And plot all the feature importances
#plt.plot(rf.steps[1][1].feature_importances_)

fig, ax = plt.subplots(figsize=(5,30))

ft_imps=rf.steps[1][1].feature_importances_
y_pos=np.arange(len(ft_imps))
ax.barh(y_pos,ft_imps,align=&#39;center&#39;)

ax.set_yticks(all_idx)
ax.set_yticklabels(all_lab)
ax.yaxis.label.set_color(&#39;red&#39;)
for i in ft_idx:
    ax.get_yticklabels()[i].set_color(&quot;red&quot;)

ax.set_xlabel(&#39;Feature Importance&#39;)

plt.show()

#plt.xticks([0,1,2,3,4,5,7,81,82,83,84,85,86])</code></pre>
<div class="figure">
<img src="01-build_the_data_files/01-build_the_data_57_0.png" alt="" />
<p class="caption">png</p>
</div>
<pre class="python"><code>#Chec the probabilities at each of the deposit/non-deposit points
print(&#39;RF...&#39;)
pRF=np.array(rf.predict_proba(features))
print(&quot;Done RF&quot;)

plt.plot(pRF[:,1])</code></pre>
<pre><code>RF...
Done RF





[&lt;matplotlib.lines.Line2D at 0x7ff04905b0b8&gt;]</code></pre>
<div class="figure">
<img src="01-build_the_data_files/01-build_the_data_58_2.png" alt="" />
<p class="caption">png</p>
</div>
</div>
<div id="finally-apply-the-model-to-the-grid" class="section level2">
<h2>Finally, apply the model to the grid</h2>
<pre class="python"><code>#Apply the trained ML to our gridded data to determine the probabilities at each of the points
print(&#39;RF...&#39;)
pRF_map=np.array(rf.predict_proba(target_data[numerical_features+categorical_features]))
print(&quot;Done RF&quot;)</code></pre>
<pre><code>RF...
Done RF</code></pre>
<pre class="python"><code>#Create a meshgrid from our xyz list of points
gridX,gridY,gridZ=grid(target_data.lon, target_data.lat,pRF_map[:,1])</code></pre>
<pre><code>/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:67: MatplotlibDeprecationWarning: The griddata function was deprecated in Matplotlib 2.2 and will be removed in 3.1. Use scipy.interpolate.griddata instead.</code></pre>
<pre class="python"><code>#Save the csv grid of targets
targetCu = {&#39;Longitude&#39;: target_data.lon, &#39;Latitude&#39;: target_data.lat, &#39;Prediction&#39;: pRF_map[:,1]}
targetCu=pd.DataFrame(targetCu)
targetCu.to_csv(&#39;Targets-&#39;+commname+&#39;.csv&#39;,header=0,index=False)</code></pre>
<pre class="python"><code>#Plot the final target map
fig = plt.figure(figsize=(10,10),dpi=300)

#Make a map projection to plot on.
ax = plt.axes(projection=ccrs.LambertAzimuthalEqualArea(central_longitude=135.0, central_latitude=-31.0))
       
#Set the extent of interest
img_extent = [min(df.LONGITUDE)+1.5,  max(df.LONGITUDE)-3.0, min(df.LATITUDE)+5,max(df.LATITUDE)-1]
ax.set_extent(img_extent)

#Put down a base map
ax.coastlines(resolution=&#39;10m&#39;, color=&#39;gray&#39;,)
tiler = Stamen(&#39;terrain-background&#39;)
mercator = tiler.crs
ax.add_image(tiler, 6)

#Make the gridlines
gl = ax.gridlines(crs=ccrs.PlateCarree(), draw_labels=True,
                  linewidth=0.3, color=&#39;gray&#39;, alpha=0.5, linestyle=&#39;-&#39;)
gl.top_labels = False
gl.bottom_labels = True
gl.right_labels = False
gl.left_labels = True
#gl.xlines = False
gl.xlocator = mticker.FixedLocator(list(np.linspace(np.floor(min(df.LONGITUDE))+1,np.ceil(max(df.LONGITUDE))-1,num=5)))
gl.ylocator = mticker.FixedLocator(list(np.linspace(np.floor(min(df.LATITUDE))+1,np.ceil(max(df.LATITUDE))-1,num=5)))
gl.xlocator = mticker.FixedLocator([141,138,135,132,129])
gl.ylocator = mticker.FixedLocator([-38,-34,-31,-28,-26])
#gl.ylocator = mticker.FixedLocator(list(np.linspace(-28,-35,num=3)))
gl.xlabel_style = {&#39;size&#39;: 10, &#39;color&#39;: &#39;gray&#39;}
gl.ylabel_style = {&#39;size&#39;: 10, &#39;color&#39;: &#39;gray&#39;}
#gl.xlabel_style = {&#39;color&#39;: &#39;red&#39;, &#39;weight&#39;: &#39;bold&#39;}
gl.xformatter = LONGITUDE_FORMATTER
gl.yformatter = LATITUDE_FORMATTER

#Create a patch of the gawler region where the data is
path=Path(list(zip(xval, yval)))
patch = PathPatch(path, facecolor=&#39;none&#39;,transform = ccrs.PlateCarree(),linestyle=&#39;--&#39;,linewidth=0.5)
plt.gca().add_patch(patch)

#Plot the main map
im=ax.contourf(gridX,gridY,gridZ,cmap=plt.cm.coolwarm,transform = ccrs.PlateCarree(),vmin=0,vmax=1)
#im = ax.imshow(gridZ, interpolation=&#39;bicubic&#39;, cmap=plt.cm.bwr,
#                origin=&#39;lower&#39;, extent=[np.min(gridX),np.max(gridX),np.min(gridY),np.max(gridY)],
#                clip_path=patch, clip_on=True,zorder=1,transform = ccrs.PlateCarree())
for c in im.collections:
    c.set_clip_path(patch)
    
# l5=ax.scatter(commall.LONGITUDE, commall.LATITUDE, 
#               edgecolor=&#39;k&#39;,s=10,marker=&#39;d&#39;, linewidths=0.5,label=&quot;&quot;,
#               c=&#39;r&#39;,cmap=plt.cm.bwr,vmin=0,vmax=1,zorder=2,transform = ccrs.PlateCarree())

#Add the deposits coloured by their classification score
l4=ax.scatter(training_data.lon[training_data.deposit==0], training_data.lat[training_data.deposit==0],
               edgecolor=&#39;k&#39;,s=20,marker=&#39;s&#39;, linewidths=1,label=&quot;&quot;,
               c=pRF[lendep:,1],cmap=plt.cm.bwr,vmin=0,vmax=1,zorder=3,transform = ccrs.PlateCarree())

l3=ax.scatter(training_data.lon[training_data.deposit==1], training_data.lat[training_data.deposit==1], 
              edgecolor=&#39;k&#39;,s=20,marker=&#39;o&#39;, linewidths=1,label=&quot;&quot;,
              c=pRF[:lendep,1],cmap=plt.cm.bwr,vmin=0,vmax=1,zorder=2,transform = ccrs.PlateCarree())

#Plot the outline of the Gawler region
ax.plot(xval,yval,&#39;k--&#39;,label=&#39;Gawler Target Region&#39;,linewidth=0.2)
ax.plot(0,0,&#39;r.&#39;,label=&#39;Known &#39;+commname+&#39; deposits for training&#39;,zorder=3,transform = ccrs.PlateCarree())
ax.plot(0,0,&#39;b.&#39;,label=&#39;Non-Deposits for training&#39;,zorder=3,transform = ccrs.PlateCarree())
#ax.plot(0,0,&#39;rd&#39;,label=&#39;All other Au deposits (not used for training)&#39;,zorder=3,transform = ccrs.PlateCarree())

# ax.plot(xlons,xlats,&#39;y-&#39;,label=&#39;Central Gawler Au Province&#39;,zorder=3,transform = ccrs.PlateCarree())
# ax.plot(xlons2,xlats2,&#39;g-&#39;,label=&#39;Olympic IOCG Province&#39;,zorder=3,transform = ccrs.PlateCarree())

# ax.plot(xval,yval,&#39;k--&#39;,label=&#39;Gawler Target Region&#39;,linewidth=0.5,zorder=2,transform = ccrs.PlateCarree())

# Add a map title, legend, colorbar
#plt.title(&#39;Known deposits and predictive map for Gawler region, SA&#39;)
ax.legend(loc=2,fontsize=12)
#plt.xlabel(&#39;Longitude&#39;)
#plt.ylabel(&#39;Latitude&#39;)

#Make a Colorbar
# cbaxes = fig.add_axes([0.16, 0.27, 0.25, 0.015])
# cbar = plt.colorbar(l3, cax = cbaxes,orientation=&quot;horizontal&quot;)
# cbar.set_label(commname+&#39; prediction&#39;)

cbaxes = fig.add_axes([0.20, 0.22, 0.1, 0.015])
cbar = plt.colorbar(im, cax = cbaxes,orientation=&quot;horizontal&quot;, ticks=[0.0,0.5,1])
# # #cbar.ax.set_xticklabels([&#39;Medium&#39;,&#39;High&#39;],fontsize=8)
cbar.set_label(commname+&#39; Prediction Score&#39;, labelpad=10,fontsize=12)
cbar.ax.xaxis.set_label_position(&#39;top&#39;)

plt.show()</code></pre>
<pre><code>&lt;urlopen error [Errno -5] No address associated with hostname&gt;
&lt;urlopen error [Errno -5] No address associated with hostname&gt;&lt;urlopen error [Errno -5] No address associated with hostname&gt;
&lt;urlopen error [Errno -5] No address associated with hostname&gt;
&lt;urlopen error [Errno -5] No address associated with hostname&gt;
&lt;urlopen error [Errno -5] No address associated with hostname&gt;</code></pre>
<div class="figure">
<img src="01-build_the_data_files/01-build_the_data_63_1.png" alt="" />
<p class="caption">png</p>
</div>
<pre class="python"><code></code></pre>
</div>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
